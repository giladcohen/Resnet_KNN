[vars]
ROOT_DIR = /data/gilad/logs/knn_bayes/wrn/mnist/log_bs_200_lr_0.1s_n_60k-SUPERSEED=23111800
TRAIN_DIR = ${vars:ROOT_DIR}/train
EVAL_DIR = ${vars:ROOT_DIR}/validation
TEST_DIR = ${vars:ROOT_DIR}/test
PREDICTION_DIR = ${vars:ROOT_DIR}/prediction
CHECKPOINT_DIR = ${vars:ROOT_DIR}/checkpoint

[self]
DEBUG_MODE = True
SUPERSEED = 23111800

[self.network]
ARCHITECTURE = Wide-Resnet-28-10
DEVICE = /gpu:0
NUM_CLASSES  = 10
IMAGE_HEIGHT = 28
IMAGE_WIDTH  = 28
NUM_RESIDUAL_UNITS = 4
EMBEDDING_DIMS = 640
NORMALIZE_EMBEDDING = True
RESNET_FILTERS = [16, 160, 320, 640]
ONE_HOT_LABELS = False

[self.network.system]
RELU_LEAKINESS = 0.1
DROPOUT_KEEP_PROB = 1.0

[self.network.optimization]
LEARNING_RATE = 0.1
XENTROPY_RATE = 1.0
WEIGHT_DECAY_RATE = 0.00078
OPTIMIZER = MOM

[self.dataset]
DATASET_NAME = mnist
TRAIN_SET_SIZE = 60000
VALIDATION_SET_SIZE = 0
TEST_SET_SIZE = 10000
USE_AUGMENTATION = True
NUM_CHANNELS = 1

[self.dataset.data_augmentation]
FLIP_IMAGE = False
DRIFT_X = 4
DRIFT_Y = 4
ZCA_NORMALIZATION = True

[self.train]

[self.train.train_control]
TRAINER = simple
TRAIN_BATCH_SIZE = 200
EVAL_BATCH_SIZE  = 2200
ROOT_DIR = ${vars:ROOT_DIR}
TRAIN_DIR = ${vars:TRAIN_DIR}
EVAL_DIR = ${vars:EVAL_DIR}
TEST_DIR = ${vars:TEST_DIR}
PREDICTION_DIR = ${vars:PREDICTION_DIR}
CHECKPOINT_DIR = ${vars:CHECKPOINT_DIR}
SUMMARY_STEPS = 10
CHECKPOINT_SECS = 600
CHECKPOINT_STEPS = [3000]
LAST_STEP = 3001
LOGGER_STEPS = 10
EVAL_STEPS = 200
TEST_STEPS = 30
RETENTION_SIZE = 5
SKIP_FIRST_EVALUATION = False
PCA_REDUCTION = True
PCA_EMBEDDING_DIMS = 64

[self.train.train_control.learning_rate_setter]
LEARNING_RATE_SETTER = fixed_schedule
SCHEDULED_STEPS = [900, 1800, 2400]
USE_FIXED_EPOCHS = False
SCHEDULED_LEARNING_RATES = [0.02, 0.004, 0.0008]

[self.test.test_control]
KNN_NEIGHBORS = 30
KNN_NORM = L1
KNN_WEIGHTS = uniform
KNN_JOBS = 20
EVAL_TRAINSET = False
COLLECT_KNN = False
COLLECT_SVM = False
COLLECT_LR = False

